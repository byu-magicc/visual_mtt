#!/usr/bin/env python
PACKAGE = "visual_mtt"

from dynamic_reconfigure.parameter_generator_catkin import *

gen = ParameterGenerator()

# INSTRUCTIONS
# Don't use this to set default values. Use yaml files, loaded in launch files.
# This cfg specifies which params are dynamically reconfigurable along with
# bounds. The reconfigure callback needs to recieve these and assign them to
# overwrite the class members.

# entries decoded:
# group.add("strict_param_name", type, level, "Mouseover Description", default, min, max)

# general processing
general = gen.add_group("General Processing")
general.add("frame_stride", int_t, 0, "Process Every Nth Frame", 0, 1, 10)
general.add("resize_scale", double_t, 0, "Scale of video for computer vision operations", 0, 0.1, 1)
general.add("published_video_scale", double_t, 0, "Scale of output video topic", 0, 0.1, 1)
general.add("published_frame_stride", int_t, 0, "Output every frame_stride + published_frame_stride frames", 0, 1, 10)
general.add("text_scale", double_t, 0, "Scale of text on output image", 0, 0, 10)

# feature manager
feature_manager = gen.add_group("Feature Manager")
feature_manager.add("parallax_comp",bool_t,0,"Enable or Disable parallax compensation on features",False)
feature_manager.add("parallax_threshold",double_t,0,"Minimum Perpendicular (to epipolar line) Velocity", 0, 0, 1.0)

# feature extractors
lkt_tracker = gen.add_group("LKT Tracker")
lkt_tracker.add("lkt_tracker_max_features", int_t,0,"Maximum Number of Features to Pair",0,0,10000)
lkt_tracker.add("lkt_tracker_enabled", bool_t,0,"Enable or Disable this feature extractor",True)

# feature motion
feature_motion = gen.add_group("Feature Motion")
feature_motion.add("feature_motion_enabled", bool_t, 0, "Enable or Disable the Feature Motion Source", False)
feature_motion.add("feature_motion_sigmaR_pos", double_t, 0, "Measurement Covariance for position data", 0, 0, 0.5)
feature_motion.add("feature_motion_sigmaR_vel", double_t, 0, "Measurement Covariance for velocity data", 0, 0, 0.5)
feature_motion.add("minimum_feature_velocity", double_t, 0, "Minimum Feature Velocity", 0, 0, 0.2)
feature_motion.add("maximum_feature_velocity", double_t, 0, "Maximum Feature Velocity", 0, 0, 0.3)

# difference image
difference_image = gen.add_group("Difference Image")
difference_image.add("difference_image_enabled", bool_t, 0, "Enable or Disable the Difference Image Source", False)
difference_image.add("difference_extra_plots", bool_t, 0, "Helpful Introspective Plots", False)
difference_image.add("difference_image_sigmaR_pos", double_t, 0, "Measurement Covariance for position data", 0, 0, 0.3)
difference_image.add("blur_kernel", int_t, 0, "Blur Kernel Size", 0, 0, 50)
difference_image.add("blur_sigma", double_t, 0, "Smoothing Standard Deviation", 0, 0, 20)
difference_image.add("threshold", double_t, 0, "Difference Threshold", 0, 0, 255)
difference_image.add("morph_size", int_t, 0, "Morphology Element Size", 0, 0, 20)
difference_image.add("morph_iterations", int_t, 0, "Morphology Iterations", 0, 1, 5)
difference_image.add("min_complexity", int_t, 0, "Minimum Difference Complexity", 0, 0, 100)
difference_image.add("max_complexity", int_t, 0, "Maximum Difference Complexity", 0, 0, 100)

# darknet plugin
darknet_plugin = gen.add_group("Darknet")
darknet_plugin.add("darknet_enabled", bool_t,0, "Enable or Disable the Darknet Source", False)
darknet_plugin.add("darknet_threshold", double_t,0, "The YOLO Net's object detection threshold", 0, 0,1)
darknet_plugin.add("darknet_frame_stride", int_t,0, "This frame stride is relative to the gereral process' frame stride.", 0,1,10)
darknet_plugin.add("darknet_draw_detections", bool_t,0,"If true, the detected results will be drawn to an image. This is good for debugging", False)

# color detector
color_detector = gen.add_group("Color Detector")
color_detector.add("color_detector_enabled", bool_t,0, "Enable or Disable the Color Detector Source", False)
color_detector.add("color_detector_min_hue", int_t,0,"Minimum Hue threshold",0,0,179)
color_detector.add("color_detector_max_hue", int_t,0,"Maximum Hue threshold",0,0,179)
color_detector.add("color_detector_min_sat", int_t,0,"Minimum Saturation threshold",0,0,255)
color_detector.add("color_detector_max_sat", int_t,0,"Maximum Saturation threshold",0,0,255)
color_detector.add("color_detector_min_val", int_t,0,"Minimum Value threshold",0,0,255)
color_detector.add("color_detector_max_val", int_t,0,"Maximum Value threshold",0,0,255)
color_detector.add("color_detector_morph_iterations", int_t,0,"The number of times the image is eroded and dilated during morphological opening and closing",0,1,5)
color_detector.add("color_detector_morph_kernel", int_t,0,"The kernel size of the erosion and dilation during morphological opening and closing",0,2,9)
color_detector.add("color_detector_min_blob_size", int_t,0,"The minimum number of pixels the color shade blob must have to be detected.",0,1,2000000)
color_detector.add("color_detector_max_blob_size", int_t,0,"The maximum number of pixels the color shade blob can have to be detected.",0,1,2000000)

# transform manager
transform_manager = gen.add_group("Transform Manager")
transform_manager.add("simple_homography_enabled", bool_t, 0,"Enable or Disable the Simple Homography Method", False)

# recognition manager
recognition_manager = gen.add_group("Target Recognition Manager")
rec_type_enum = gen.enum([ gen.const("None",                int_t, 0, ""),
                           gen.const("Template_Matching",   int_t, 1, ""),
                           gen.const("Visual_Bag_of_Words", int_t, 2, "")], "")
recognition_manager.add("recognition_type", int_t, 0, "Recognition Type Used for Track Reacquisition", 0, edit_method=rec_type_enum)
recognition_manager.add("crop_width", int_t, 0, "Cropped Subimage Width Used for Recognition", 0, 1, 200)


exit(gen.generate(PACKAGE, "visual_frontend", "visual_frontend"))